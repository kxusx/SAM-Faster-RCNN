{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf06d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision.ops import roi_align\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Load SAM and get masks\n",
    "# ---------------------------\n",
    "def load_sam(model_type=\"vit_h\", checkpoint_path=\"sam_vit_h.pth\"):\n",
    "    sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
    "    predictor = SamPredictor(sam)\n",
    "    return predictor\n",
    "\n",
    "def get_sam_boxes(predictor, image):\n",
    "    predictor.set_image(image)\n",
    "    masks, _, _ = predictor.predict(multimask_output=True)\n",
    "\n",
    "    boxes = []\n",
    "    for mask in masks:\n",
    "        pos = np.where(mask)\n",
    "        if pos[0].size == 0 or pos[1].size == 0:\n",
    "            continue\n",
    "        y_min, x_min = np.min(pos[0]), np.min(pos[1])\n",
    "        y_max, x_max = np.max(pos[0]), np.max(pos[1])\n",
    "        if (x_max - x_min) * (y_max - y_min) > 500:  # Filter small regions\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "    return boxes  # list of [x1, y1, x2, y2]\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: Load Backbone\n",
    "# ---------------------------\n",
    "def get_backbone_feature_map(image_tensor, device=\"cuda\"):\n",
    "    backbone = torchvision.models.resnet50(pretrained=True)\n",
    "    backbone = torch.nn.Sequential(*list(backbone.children())[:-2])  # Remove FC and avgpool\n",
    "    backbone = backbone.to(device)  # ðŸ”¥ Move model to same device as input\n",
    "    backbone.eval()\n",
    "    with torch.no_grad():\n",
    "        feature_map = backbone(image_tensor.to(device))  # ðŸ”¥ Also move input to device\n",
    "    return feature_map\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Step 3: Apply ROI Align\n",
    "# ---------------------------\n",
    "def apply_roi_align(feature_map, boxes, image_size, output_size=(7, 7)):\n",
    "    # Convert boxes to (index, x1, y1, x2, y2) format required by roi_align\n",
    "    boxes_tensor = torch.tensor(boxes, dtype=torch.float32)\n",
    "    batch_indices = torch.zeros((boxes_tensor.shape[0], 1))\n",
    "    boxes_tensor = torch.cat([batch_indices, boxes_tensor], dim=1)  # Add batch index\n",
    "\n",
    "    # Normalize box coordinates to feature map scale\n",
    "    scale_y = feature_map.shape[2] / image_size[0]\n",
    "    scale_x = feature_map.shape[3] / image_size[1]\n",
    "    boxes_tensor[:, 1::2] *= scale_x\n",
    "    boxes_tensor[:, 2::2] *= scale_y\n",
    "\n",
    "    # Move boxes to the same device as feature_map\n",
    "    boxes_tensor = boxes_tensor.to(feature_map.device)\n",
    "\n",
    "    roi_features = roi_align(feature_map, boxes_tensor, output_size=output_size)\n",
    "    return roi_features\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Step 4: Classification Head\n",
    "# ---------------------------\n",
    "class DetectionHead(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.head = FastRCNNPredictor(in_channels, num_classes)\n",
    "\n",
    "    def forward(self, roi_feats):\n",
    "        roi_feats = torch.flatten(roi_feats, start_dim=1)\n",
    "        return self.head(roi_feats)\n",
    "\n",
    "# ---------------------------\n",
    "# Full Pipeline\n",
    "# ---------------------------\n",
    "def run_pipeline(image_path, sam_checkpoint, device=\"cuda\"):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_tensor = torchvision.transforms.functional.to_tensor(image_rgb).unsqueeze(0).to(device)\n",
    "\n",
    "    predictor = load_sam(checkpoint_path=sam_checkpoint)\n",
    "    boxes = get_sam_boxes(predictor, image_rgb)\n",
    "\n",
    "    if not boxes:\n",
    "        print(\"No valid boxes found by SAM.\")\n",
    "        return\n",
    "\n",
    "    feature_map = get_backbone_feature_map(image_tensor).to(device)\n",
    "    roi_feats = apply_roi_align(feature_map, boxes, image_rgb.shape[:2])\n",
    "\n",
    "    # Use dummy head for now; in real training, replace with trained head\n",
    "    detection_head = DetectionHead(in_channels=roi_feats.shape[1] * 7 * 7, num_classes=91).to(device)\n",
    "    outputs = detection_head(roi_feats)\n",
    "\n",
    "    # save the image\n",
    "    cv2.imwrite(\"output.jpg\", image_rgb)\n",
    "\n",
    "    print(\"Predicted class scores:\", outputs)\n",
    "\n",
    "# Example usage\n",
    "run_pipeline(\"rail.jpg\", sam_checkpoint=\"sam_vit_h.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
